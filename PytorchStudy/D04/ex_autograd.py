## ---------------------------------------------------
## Autograd 동작 원리 이해
## - 기울기를 이용해 변수 값 자동 계산 기능  
## - .backward() 메서드   : 기울기 계산 
## - .grad 속성           : 계산된 기울기 저장
## - .grad.zero_() 메서드 : 기존에 저장된 grad값 초기화
## ---------------------------------------------------

## ---------------------------------------------------
## 모듈 로딩
## ---------------------------------------------------
import torch

## ---------------------------------------------------
## 업데이트 간격 설정 변수 선언
## ---------------------------------------------------
# 학습률
_lr = 0.1


## ---------------------------------------------------
## Tensor 생성 
## ---------------------------------------------------
# 초기 값 설정 (학습할 파라미터 w)
w = torch.tensor(1.0, requires_grad=True)

# 학습 데이터
x = torch.tensor(1.0)       # 입력
y_true = torch.tensor(2.0)  # 목표 출력 (정답)


## ---------------------------------------------------
## Tensor 값 자동 업데이트 
## ---------------------------------------------------
print("-" * 58)
print(f"{'Epoch':^5} | {'w':^8} | {'Loss':^10} | {'grad':^10} | {'y_pred':^10} | {'y_true':^10}")
print("-" * 58)


for epoch in range(1, 21):
    # 순전파: 예측값 계산
    y_pred = w * x

    # 손실(loss): (예측 - 정답)^2
    loss = (y_pred - y_true) ** 2

    # 역전파: gradient 계산 즉, 미분 진행 
    loss.backward()

    # 현재 값, 손실, 기울기 출력
    print(f"{epoch:^5} | {w.item():^8.4f} | {loss.item():^10.6f} | {w.grad.item():^10.6f}| {y_pred:^10.6f} | {y_true.item():^10.6f}")

    # 값 업데이트: w = w - lr * grad
    with torch.no_grad():
        w -= _lr * w.grad

    # gradient 초기화 (누적 방지)
    w.grad.zero_()