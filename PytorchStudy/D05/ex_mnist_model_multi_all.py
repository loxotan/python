## -------------------------------------------------------------
## ëª©í‘œ : MNIST DIGIT HANDWRITTEN ì‹ë³„í•˜ëŠ” ëª¨ë¸ ê°œë°œ
## -------------------------------------------------------------
## - ì¡°ì‚¬  ê²°ê³¼ : 28x28 í¬ê¸°ì˜ ìˆ«ì ì†ê¸€ì”¨ ì´ë¯¸ì§€ ë°ì´í„°, í‘ë°±
## - ë°ì´í„°ìˆ˜ì§‘ : mnist_train.csv, mnist_test.csv
## - ë°ì´í„°ë¶„ì„ : 0 ~ 9 ì¦‰, 10ê°œ ìˆ«ì ì´ë¯¸ì§€ ë¶„ë¥˜ 
##
## - ë°ì´í„°  ì…‹ : í•™ìŠµìš©, ê²€ì¦ìš©, í…ŒìŠ¤íŠ¸ìš© ì¤€ë¹„
## - ëª¨ë¸  ê°œë°œ : ë¶„ë¥˜ ëª¨ë¸ => ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ  => í•™ìŠµ ì§„í–‰
##               ì¸ê³µì‹ ê²½ë§ ì•Œê³ ë¦¬ì¦˜ => DNN ëª¨ë¸
## -------------------------------------------------------------

## -------------------------------------------------------------
## ëª¨ë“ˆ ë¡œë”©
## -------------------------------------------------------------
## - ë°ì´í„° ë¶„ì„ ë° ë¡œë”©
import pandas as pd    

## - í…ì„œ, ì¸ê³µì‹ ê²½ë§, ìµœì í™” ê´€ë ¨ ëª¨ë“ˆ
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

## - ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í™œìš© ëª¨ë“ˆ
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

## - ë°ì´í„°ì…‹ê³¼ ë°ì´í„°ë¡œë” ê´€ë ¨ ëª¨ë“ˆ
from torch.utils.data import Dataset, DataLoader
import numpy as np

## - í•™ìŠµê²°ê³¼ ì‹œê°í™” ë° í‰ê°€ ì§€í‘œ
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


## -------------------------------------------------------------
## ë°ì´í„° ì¤€ë¹„
## -------------------------------------------------------------
##- ë°ì´í„° ë¡œë”©
DATA_TRAIN_FILE = '../data/mnist_train.csv'
DATA_TEST_FILE  = '../data/mnist_test.csv'

##- csv íŒŒì¼ ì²«ë²ˆì§¸ ì¤„ì´ ì»¬ëŸ¼ëª… ì•„ë‹˜ => header=None
trainDF= pd.read_csv(DATA_TRAIN_FILE, header=None)
testDF = pd.read_csv(DATA_TEST_FILE, header=None)


## -------------------------------------------------------------
## í”¼ì³ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
## -------------------------------------------------------------
##- í”¼ì³ : ì´ë¯¸ì§€ í”½ì…€ ê°’
X_train_all = trainDF.iloc[:, 1:] / 255.0
X_test      = testDF.iloc[:, 1:]  / 255.0

##- íƒ€ê²Ÿ : ì´ë¯¸ì§€ê°€ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì
y_train_all = trainDF.iloc[:, 0 ]
y_test      = testDF.iloc[:, 0 ]

##- í”¼ì³ì™€ íƒ€ê²Ÿ ì²´í¬
print(f'[TRAIN ALL] í”¼ì³ : {X_train_all.shape}  íƒ€ê²Ÿ : {y_train_all.shape}')
print(f'[   TEST  ] í”¼ì³ : {X_test.shape}  íƒ€ê²Ÿ : {y_test.shape}')
## -------------------------------------------------------------
## ì „ì²˜ë¦¬ => í”¼ì³ ì •ê·œí™” 
## -------------------------------------------------------------
## - í”½ì…€ê°’ 0 ~ 255 ==> 0.0 ~ 1.0  : í•™ìŠµ ì•ˆì •ì„± ë° ëª¨ë¸ ì¼ë°˜í™” 
featureDF = X_train_all/255.
print(featureDF.head(3))

## -------------------------------------------------------------
## í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¶„í• 
## -------------------------------------------------------------
X_train, X_val, y_train, y_val = train_test_split(  X_train_all, 
                                                    y_train_all, 
                                                    test_size=0.25, 
                                                    random_state=42, 
                                                    stratify=y_train_all)


print(f"Train label distribution: {np.bincount(y_train)}" )
print(f"Val label distribution  : {np.bincount(y_val)}")

## -------------------------------------------------------------
## ì»¤ìŠ¤í…€ ëª¨ë¸ ì •ì˜
## -------------------------------------------------------------
class MnistDataset(Dataset):
    def __init__(self, features, labels):
        self.x = features
        self.y = labels

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        featrue = torch.tensor(self.x[idx], dtype=torch.float32)
        label = torch.tensor(self.y[idx], dtype=torch.long)
        return featrue, label   

## -------------------------------------------------------------
## ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ëª¨ë¸ ì •ì˜
## -------------------------------------------------------------   
class MnistClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )

    def forward(self, x):
        return self.net(x)
    
## -------------------------------------------------------------
## ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ 
## -------------------------------------------------------------
## - í•¨ìˆ˜ê¸°ëŠ¥ : 1ì—í¬í¬ í•™ìŠµ í›„ loss ë°˜í™˜
## - í•¨ìˆ˜ì´ë¦„ : train
## - ë§¤ê°œë³€ìˆ˜ : model, dataloader, criterion, optimizer
## - í•¨ìˆ˜ê²°ê³¼ : loss ë°˜í™˜
## -------------------------------------------------------------
def train(model, dataloader, criterion, optimizer):
    #- ëª¨ë¸ ë™ì‘ ëª¨ë“œ ì„¤ì • : í•™ìŠµëª¨ë“œ
    model.train()

    #- ë°°ì¹˜í¬ê¸°ë§Œí¼ ì „ë°©í–¥ í•™ìŠµ & ì—­ì „íŒŒ 
    total_loss = 0    # 1ì—í¬í¬(ì²˜ìŒë¶€í„° ëê¹Œì§€ í•™ìŠµ)ì— ëŒ€í•œ ì „ì²´ ì†ì‹¤ì €ì¥
    for x_batch, y_batch in dataloader:
        #- ì „ë°©í–¥(forward) í•™ìŠµ
        optimizer.zero_grad()
        outputs = model(x_batch)

        #- ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ì°¨ì´ ê³„ì‚° ì¦‰, ì†ì‹¤/ë¹„ìš©í•¨ìˆ˜ ì²˜ë¦¬
        loss = criterion(outputs, y_batch)

        #- ì†ì‹¤ê°’ì„ ë¯¸ë¶„ ì§„í–‰ 
        loss.backward()
        #- ëª¨ë¸ì˜ W, bì— ìƒˆë¡œìš´ ê°’ ì—…ë°ì´íŠ¸
        optimizer.step()
        #- ë°°ì¹˜í¬ê¸°ë§Œí¼ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ ì¶”ê°€ ì €ì¥
        total_loss += loss.item()

    return total_loss / len(dataloader)

## -------------------------------------------------------------
## - í•¨ìˆ˜ê¸°ëŠ¥ : í˜„ì¬ ëª¨ë¸ì˜ W, bë¡œ ê²€ì¦ë°ì´í„°ì— ëŒ€í•œ ê²€ì¦ í›„ loss ë°˜í™˜
## - í•¨ìˆ˜ì´ë¦„ : evaluate
## - ë§¤ê°œë³€ìˆ˜ : model, dataloader, criterion
## - í•¨ìˆ˜ê²°ê³¼ : loss, ì •í™•ë„ ë°˜í™˜
## -------------------------------------------------------------
def evaluate(model, dataloader, criterion):
    ## - ëª¨ë¸ ë™ì‘ ëª¨ë“œ ì„¤ì • : ê²€ì¦ ëª¨ë“œ
    model.eval()

    ## - ê²€ì¦ë°ì´í„°ì…‹ì— ëŒ€í•œ ì†ì‹¤, ì •í™•ë„ ì €ì¥ ë³€ìˆ˜
    total_loss = 0
    correct = 0
    total = 0

    ## - ê²€ì¦ë°ì´í„°ì…‹ìœ¼ë¡œ ê²€ì¦ ì§„í–‰
    with torch.no_grad():
        for x_batch, y_batch in dataloader:
            ##- ì˜ˆì¸¡ê°’ ì¶”ì¶œ
            outputs = model(x_batch)

            ##- ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ì°¨ì´ ê³„ì‚° 
            loss = criterion(outputs, y_batch)

            ##- ì°¨ì´ ì¦‰, ì†ì‹¤ ëˆ„ì 
            total_loss += loss.item()

            ##- ì˜ˆì¸¡ íƒ€ê²Ÿ ì¶”ì¶œ
            preds = outputs.argmax(dim=1)
            ##- ì •ë‹µ íƒ€ê²Ÿê³¼ ì˜ˆì¸¡ íƒ€ê²Ÿ ë¹„êµ ë° ì¹´ìš´íŒ…
            correct += (preds == y_batch).sum().item()
            total += y_batch.size(0)

    accuracy = correct / total
    return total_loss / len(dataloader), accuracy

## -------------------------------------------------------------
## - í•¨ìˆ˜ê¸°ëŠ¥ : ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ë°˜í™˜
## - í•¨ìˆ˜ì´ë¦„ : evaluate
## - ë§¤ê°œë³€ìˆ˜ : model, dataloader, criterion
## - í•¨ìˆ˜ê²°ê³¼ : loss, ì •í™•ë„ ë°˜í™˜
## -------------------------------------------------------------
def predict(model, data):
    model.eval()
    with torch.no_grad():
        if isinstance(data, torch.Tensor):
            x_tensor = data.clone().detach()
        else:
            x_tensor = torch.tensor(data, dtype=torch.float32)

        outputs = model(x_tensor)
        predictions = outputs.argmax(dim=1)
    return predictions


## --------------------------------------------------------------------
## í•™ìŠµ ì¤€ë¹„
## --------------------------------------------------------------------
## - í•™ìŠµ ì§„í–‰ ê´€ë ¨ ì„¤ì •ê°’
_LR         = 0.01
_BATCH_SIZE = 16
EPOCHS      = 51
step_cnt = 5

## - ë°ì´í„°ë¡œë” ê°ì²´ ìƒì„± 
train_loader = DataLoader(MnistDataset(X_train.values, y_train.values), batch_size=_BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(MnistDataset(X_val.values, y_val.values), batch_size=_BATCH_SIZE)
test_loader  = DataLoader(MnistDataset(X_test.values, y_test.values), batch_size=_BATCH_SIZE)

## - í•™ìŠµê´€ë ¨ ê°ì²´ ìƒì„±
model        = MnistClassifier()                      ## - ëª¨ë¸ ê°ì²´ ìƒì„±
criterion    = nn.CrossEntropyLoss()                  ## - ì†ì‹¤ í•¨ìˆ˜ ê°ì²´ ìƒì„±
optimizer    = optim.Adam(model.parameters(), lr=_LR) ## - ìµœì í™” ê°ì²´ ìƒì„± 
scheduler    = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', factor=0.1, patience=5)  ## - í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
## - í•™ìŠµê²°ê³¼ ì €ì¥ ë³€ìˆ˜ë“¤
train_losses, val_losses, val_accuracies = [], [], []


## --------------------------------------------------------------------
## í•™ìŠµ ì§„í–‰
## --------------------------------------------------------------------
for epoch in range(1, EPOCHS):
    ## í•™ìŠµ í›„ ì†ì‹¤ 
    train_loss = train(model, train_loader, criterion, optimizer)

    ## ê²€ì¦ í›„ ì†ì‹¤ ë° ì„±ëŠ¥ 
    val_loss, val_acc = evaluate(model, val_loader, criterion)

    ## Epochë‹¨ìœ„ í•™ìŠµ/ê²€ì¦ ê²°ê³¼ ì €ì¥
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    val_accuracies.append(val_acc)
    scheduler.step(val_acc)
    
    if scheduler.num_bad_epochs >= scheduler.patience:
        step_cnt -= 1
        print(f'step count remaining: {step_cnt}, learning rate reduced')
    
    if step_cnt == 0:
        print(f'stopping early at epoch {epoch} due to no improvement')
        break
    
    ## 10íšŒ ë§ˆë‹¤ ì¶œë ¥
    if epoch % 10 == 0 or epoch == 1:
        print(f"Epoch {epoch:2d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")


## -------------------------------------------------------------
## í…ŒìŠ¤íŠ¸ í‰ê°€ ë° Confusion Matrix
## -------------------------------------------------------------
test_loss, test_acc = evaluate(model, test_loader, criterion)
print(f"Test Accuracy: {test_acc:.4f}")


## ===> Confusion Matrix
all_preds = []
all_labels = []
model.eval()
with torch.no_grad():
    for x_batch, y_batch in test_loader:
        preds = model(x_batch).argmax(dim=1)
        all_preds.extend(preds.numpy())
        all_labels.extend(y_batch.numpy())

cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(cm)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()


# -------------------------------------------------------------
# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
# -------------------------------------------------------------
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.legend()
plt.title("Loss per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid()
plt.show()

plt.plot(val_accuracies, label='Validation Accuracy', color='green')
plt.legend()
plt.title("Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.grid()
plt.show()

# -------------------------------------------------------------
# ìƒˆë¡œìš´ ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
# -------------------------------------------------------------
sample = X_test.iloc[[0]].values
pred = predict(model, sample)
print(f"ì˜ˆì¸¡ ê²°ê³¼: {pred.item()}, ì‹¤ì œ ì •ë‹µ: {y_test.iloc[0]}")


# -------------------------------------------------------------
# AccuracyëŠ” ë§ì·„ì§€ë§Œ Lossê°€ ë†’ì€ ìƒ˜í”Œ ì¶”ì¶œ ì˜ˆì‹œ
# -------------------------------------------------------------
# ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë¹„êµ (MNIST ê¸°ì¤€)
def find_high_loss_correct_predictions(model, dataloader, criterion, num_samples=5):
    model.eval()
    results = []
    with torch.no_grad():
        for x_batch, y_batch in dataloader:
            outputs = model(x_batch)
            preds = outputs.argmax(dim=1)
            loss_per_sample = F.cross_entropy(outputs, y_batch, reduction='none')

            for i in range(len(y_batch)):
                if preds[i] == y_batch[i]:
                    results.append((loss_per_sample[i].item(), preds[i].item(), y_batch[i].item(), x_batch[i]))

    results.sort(reverse=True, key=lambda x: x[0])  # loss ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
    print(f"ğŸ“Œ AccuracyëŠ” ë§ì•˜ì§€ë§Œ Lossê°€ ë†’ì€ ìƒ˜í”Œ {num_samples}ê°œ")
    for i in range(min(num_samples, len(results))):
        loss, pred, true, _ = results[i]
        print(f"Sample {i+1}: Loss={loss:.4f}, ì˜ˆì¸¡={pred}, ì •ë‹µ={true}")


find_high_loss_correct_predictions(model, val_loader, criterion)
